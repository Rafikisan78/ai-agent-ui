# Guide: Compl√©ter le Workflow Voice-Text-Video dans N8N

## ‚úÖ Workflow de Base Cr√©√©

- **ID**: `EM3TcglVa2ngfwRF`
- **URL**: https://n8n.srv766650.hstgr.cloud/workflow/EM3TcglVa2ngfwRF
- **Webhook**: `https://n8n.srv766650.hstgr.cloud/webhook/voice-text-video`

---

## üîß √âtapes pour Compl√©ter le Workflow

### √âtape 1: Ouvrir le Workflow

1. Allez sur: https://n8n.srv766650.hstgr.cloud/workflow/EM3TcglVa2ngfwRF
2. Vous verrez 3 n≈ìuds de base:
   - Webhook
   - Analyze Request
   - Respond to Webhook

---

### √âtape 2: Ajouter le Switch pour Router

1. **Supprimer** la connexion entre "Analyze Request" et "Respond to Webhook"
2. **Ajouter** un n≈ìud **"Switch"** apr√®s "Analyze Request"
3. **Configurer** le Switch:
   - Cliquer sur le n≈ìud Switch
   - Mode: "Rules"
   - Ajouter 4 conditions:

     **Condition 1 - Voice**:
     - Type: String
     - Value 1: `={{ $json.requestType }}`
     - Operation: equals
     - Value 2: `voice`

     **Condition 2 - Text**:
     - Type: String
     - Value 1: `={{ $json.requestType }}`
     - Operation: equals
     - Value 2: `text`

     **Condition 3 - Image**:
     - Type: String
     - Value 1: `={{ $json.requestType }}`
     - Operation: equals
     - Value 2: `image`

     **Condition 4 - Video**:
     - Type: String
     - Value 1: `={{ $json.requestType }}`
     - Operation: equals
     - Value 2: `video`

---

### √âtape 3: Branche VOICE - Transcription Audio

#### 3.1 Ajouter "Function" pour pr√©parer l'audio

1. **Connecter** la sortie 0 du Switch (voice)
2. **Ajouter** un n≈ìud **"Function"**
3. **Nommer**: "Prepare Audio"
4. **Code**:
```javascript
// Conversion base64 vers buffer pour Whisper
const data = $input.first().json;
console.log('üé§ Audio re√ßu, taille:', data.audio_data?.length || 0);

if (!data.audio_data) {
  throw new Error('Aucune donn√©e audio');
}

// D√©coder le base64
const audioBuffer = Buffer.from(data.audio_data, 'base64');
console.log('üìä Buffer cr√©√©:', audioBuffer.length, 'bytes');

return {
  json: {
    audioBuffer: audioBuffer.toString('base64'),
    format: data.format || 'webm'
  },
  binary: {
    data: {
      data: audioBuffer,
      mimeType: 'audio/webm',
      fileName: 'voice.webm'
    }
  }
};
```

#### 3.2 Ajouter "OpenAI" pour Whisper

1. **Ajouter** un n≈ìud **"OpenAI"**
2. **Nommer**: "Whisper Transcription"
3. **Configurer**:
   - Resource: "Audio"
   - Operation: "Transcribe"
   - Binary Property: `data`
   - Language: `fr` (ou laissez vide pour auto-d√©tection)
   - Model: `whisper-1`
4. **Credentials**: S√©lectionner "OpenAI Account" (existant)

#### 3.3 Ajouter "Function" pour extraire le texte

1. **Ajouter** un n≈ìud **"Function"**
2. **Nommer**: "Extract Transcription"
3. **Code**:
```javascript
const data = $input.first().json;
const transcription = data.text || '';

console.log('‚úÖ Transcription:', transcription);

return {
  json: {
    message: transcription,
    source: 'voice',
    timestamp: new Date().toISOString()
  }
};
```

---

### √âtape 4: Branche TEXT - Traitement Direct

1. **Connecter** la sortie 1 du Switch (text)
2. **Ajouter** un n≈ìud **"Function"**
3. **Nommer**: "Process Text"
4. **Code**:
```javascript
const data = $input.first().json;
console.log('üí¨ Texte:', data.message);

return {
  json: {
    message: data.message,
    source: 'text',
    timestamp: new Date().toISOString()
  }
};
```

---

### √âtape 5: Merger Voice & Text

1. **Ajouter** un n≈ìud **"Merge"**
2. **Nommer**: "Merge Voice & Text"
3. **Configurer**:
   - Mode: "Combine"
   - Combine By: "Merge By Position"
4. **Connecter**:
   - Input 1: "Extract Transcription"
   - Input 2: "Process Text"

---

### √âtape 6: Analyser le Type de Demande

1. **Ajouter** un n≈ìud **"Function"** apr√®s le Merge
2. **Nommer**: "Detect Image/Video"
3. **Code**:
```javascript
const data = $input.first().json;
const message = data.message || '';

const isImage = message.toLowerCase().includes('/image');
const isVideo = message.toLowerCase().includes('/video');

let prompt = message;
if (isImage) prompt = message.replace('/image', '').trim();
if (isVideo) prompt = message.replace('/video', '').trim();

console.log('üîç Type:', isImage ? 'image' : isVideo ? 'video' : 'text');
console.log('üìù Prompt:', prompt);

return {
  json: {
    prompt: prompt,
    type: isImage ? 'image' : isVideo ? 'video' : 'text',
    source: data.source,
    originalMessage: message
  }
};
```

---

### √âtape 7: Router Final (Text/Image/Video)

1. **Ajouter** un n≈ìud **"Switch"**
2. **Nommer**: "Route Content Type"
3. **Configurer** 3 conditions:
   - Condition 1: `{{ $json.type }}` equals `text`
   - Condition 2: `{{ $json.type }}` equals `image`
   - Condition 3: `{{ $json.type }}` equals `video`

---

### √âtape 8: Branche IMAGE - DALL-E

1. **Connecter** la sortie 1 du Router (image)
2. **Ajouter** un n≈ìud **"OpenAI"**
3. **Nommer**: "DALL-E Generate"
4. **Configurer**:
   - Resource: "Image"
   - Operation: "Generate"
   - Prompt: `={{ $json.prompt }}`
   - Size: "1024x1024"
   - Number of Images: 1
5. **Credentials**: "OpenAI Account"

#### 8.1 Formater la r√©ponse image

1. **Ajouter** un n≈ìud **"Function"**
2. **Nommer**: "Format Image Response"
3. **Code**:
```javascript
const data = $input.first().json;
const imageUrl = data.data?.[0]?.url;

console.log('üñºÔ∏è Image:', imageUrl);

return {
  json: {
    type: 'image',
    response: 'Image g√©n√©r√©e avec succ√®s',
    image_url: imageUrl,
    prompt: $('Detect Image/Video').item.json.prompt,
    source: $('Detect Image/Video').item.json.source
  }
};
```

---

### √âtape 9: Branche TEXT - ChatGPT

1. **Connecter** la sortie 0 du Router (text)
2. **Ajouter** un n≈ìud **"OpenAI"**
3. **Nommer**: "ChatGPT"
4. **Configurer**:
   - Resource: "Chat"
   - Model: "gpt-4" ou "gpt-3.5-turbo"
   - Messages: User Message
   - Text: `={{ $json.prompt }}`
5. **Credentials**: "OpenAI Account"

#### 9.1 Formater la r√©ponse texte

1. **Ajouter** un n≈ìud **"Function"**
2. **Nommer**: "Format Text Response"
3. **Code**:
```javascript
const data = $input.first().json;
const response = data.choices?.[0]?.message?.content || data.text;

console.log('üí¨ R√©ponse:', response);

return {
  json: {
    type: 'text',
    response: response,
    prompt: $('Detect Image/Video').item.json.prompt,
    source: $('Detect Image/Video').item.json.source
  }
};
```

---

### √âtape 10: Branche VIDEO - Replicate

1. **Connecter** la sortie 2 du Router (video)
2. **Ajouter** un n≈ìud **"HTTP Request"**
3. **Nommer**: "Replicate Video"
4. **Configurer**:
   - Method: POST
   - URL: `https://api.replicate.com/v1/predictions`
   - Authentication: Predefined Credential Type
   - Credential Type: Replicate API
   - Send Headers: ON
     - Name: `Content-Type`, Value: `application/json`
   - Send Body: ON
     - Content Type: JSON
     - Body:
     ```json
     {
       "version": "9f747673945c62801b13b84701c783929c0ee784e4748ec062204894dda1a351",
       "input": {
         "prompt": "={{ $json.prompt }}"
       }
     }
     ```
5. **Credentials**: "Replicate API" (existant)

#### 10.1 Formater la r√©ponse vid√©o

1. **Ajouter** un n≈ìud **"Function"**
2. **Nommer**: "Format Video Response"
3. **Code**:
```javascript
const data = $input.first().json;

console.log('üé¨ Vid√©o task:', data.id);

return {
  json: {
    type: 'video',
    response: 'Vid√©o en cours de g√©n√©ration',
    task_id: data.id,
    status: 'processing',
    prompt: $('Detect Image/Video').item.json.prompt,
    source: $('Detect Image/Video').item.json.source
  }
};
```

---

### √âtape 11: Merger Toutes les R√©ponses

1. **Ajouter** un n≈ìud **"Merge"**
2. **Nommer**: "Merge All Responses"
3. **Connecter**:
   - Input 0: "Format Text Response"
   - Input 1: "Format Image Response"
   - Input 2: "Format Video Response"

---

### √âtape 12: Connecter au Webhook Response

1. **Connecter** "Merge All Responses" √† "Respond to Webhook"

---

### √âtape 13: Activer le Workflow

1. Cliquer sur le **toggle** en haut √† droite
2. Le workflow devient **actif** (vert)

---

## üìä Architecture Finale

```
Webhook
  ‚Üì
Analyze Request
  ‚Üì
Switch (voice/text/image/video)
  ‚îú‚îÄ [0] Voice ‚Üí Prepare Audio ‚Üí Whisper ‚Üí Extract ‚Üí Merge
  ‚îú‚îÄ [1] Text ‚Üí Process Text ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üë
  ‚îú‚îÄ [2] Image ‚Üí (trait√© plus tard)
  ‚îî‚îÄ [3] Video ‚Üí (trait√© plus tard)
              ‚Üì
        Merge Voice & Text
              ‚Üì
        Detect Image/Video
              ‚Üì
        Route Content Type
         ‚îú‚îÄ [0] Text ‚Üí ChatGPT ‚Üí Format Text ‚Üí Merge All
         ‚îú‚îÄ [1] Image ‚Üí DALL-E ‚Üí Format Image ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üë
         ‚îî‚îÄ [2] Video ‚Üí Replicate ‚Üí Format Video ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üë
                     ‚Üì
              Merge All Responses
                     ‚Üì
              Respond to Webhook
```

---

## ‚úÖ Checklist Finale

- [ ] Tous les n≈ìuds sont ajout√©s
- [ ] Toutes les connexions sont faites
- [ ] Les credentials OpenAI sont configur√©s
- [ ] Les credentials Replicate sont configur√©s
- [ ] Le workflow est activ√© (toggle vert)
- [ ] Test avec curl:

```bash
curl -X POST https://n8n.srv766650.hstgr.cloud/webhook/voice-text-video \
  -H "Content-Type: application/json" \
  -d '{"message": "Bonjour", "type": "text"}'
```

---

## üîß Troubleshooting

### Erreur: "Missing credentials"
‚Üí V√©rifiez que vos credentials OpenAI et Replicate sont bien configur√©s dans N8N

### Erreur: "Audio transcription failed"
‚Üí V√©rifiez que le format audio est correct (webm, mp3, wav accept√©s)

### Le workflow ne r√©pond pas
‚Üí V√©rifiez qu'il est bien actif (toggle vert)
‚Üí Regardez les logs d'ex√©cution dans N8N (onglet "Executions")

---

## üéØ Prochaine √âtape

Une fois le workflow compl√©t√© et actif, passez √† la mise √† jour de l'application web pour ajouter le bouton microphone!
